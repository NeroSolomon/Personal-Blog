## playwright mcp
mcp-playwright是一个使用 Playwright 提供浏览器自动化能力的模型上下文协议服务器。该服务器使 LLMs 能够与网页交互、截屏，并在真实的浏览器环境中执行 JavaScript。


https://zhuanlan.zhihu.com/p/31352418142

vscode cline配置playwright mcp：https://linux.do/t/topic/574685

## ai agent
Agent = LLM + 规划技能 + 记忆 + 工具使用

https://zhuanlan.zhihu.com/p/657937696

## Claude Code长期记忆：优雅的上下文管理

1. 短期记忆（Short-Term Memory）

    - 实现方式：一个简单的消息队列（Message Queue），保证对最新消息的 O(1) 访问效率。

    - 核心功能：实时追踪当前对话的 Token 使用量，为后续的压缩决策提供依据。

2. 中期记忆（Mid-Term Memory）

    当短期记忆的使用率触及 92% 的阈值时，中期记忆层便会启动。它的核心是 wU2 压缩器，其工作不是粗暴地丢弃数据，而是进行“智能蒸馏”——调用一个专门的 LLM，将冗长的对话历史提炼成一份结构化的摘要。

3. 第3层：长期记忆（Long-Term Memory）

    长期记忆是跨会话（cross-session）的知识存储层，通常以一个 CLAUDE.md 文件的形式存在。它存储的是那些经过中期记忆提炼后，被认为具有长期价值的信息，比如用户偏好、项目配置、通用解决方案等。

    这一层不仅仅是简单的文件读写，更重要的是它支持向量化搜索（Vector Search）。当新的对话开始时，系统可以将用户的问题转换成向量，在长期记忆库中进行相似度检索，从而“回忆”起过去相关的经验，让 AI 具备跨越时间窗口解决问题的能力。

## Embedding
什么是Embedding：https://zhuanlan.zhihu.com/p/667877230

## RAG应用
RAG（Retrieval-Augmented Generation，检索增强生成）技术

### 文档处理
切片->向量化->存储

### 切分方式
- markdown
    - split-md：按标题与token进行切分，自动进行上下文拓展与超长截断，适合标题间内容关联相对紧密的文档
    - split-md-2nd：按二级标题完整切分，若标题下内容超长会截断并附加标题，适合每个二级标题内容相对独立的文档
- Excel
    - split-excel-token：按token进行切分，以从左至右从上至下拼接sheet内容，适用于文本文档
    - split-excel-row：按行进行切分，包含表头，适用于FAQ文档等一行一条数据的场景
    - split-excel-block ：按内容块进行切分，根据填充色识别块与表头

### 问答检索配置
- 命中片段：当次问答检索的切片数量，数量越大，模型拿到的信息越全，但也可以影响总结效果，可根据实际情况选用；由于模型有token数限制，因此当片段内容过多时，会自动从后往前删减片段（最大是50）
    若问题的相关信息比较集中，建议调小，避免信息混淆
    若问题的相关信息比较分散，建议调大，避免片段丢失
- 输出模式：选择问答时模型输出的内容，即选择是否要经过大语言模型总结
    总结模式（推荐）：把 检索到的片段+输入 发送给大模型进行结果加工，支持选择内置提示词或自定义
    普通模式：直接返回检索到的文档片段（PS：切片会清洗掉一些特殊字符，不推荐使用）
- 输入模式：检索前是否需要对输入进行处理
    预处理模式：在文档检索前根据定义的任务对输入进行处理，如翻译、问题简化等
        简化问题：将输入转化成一个简单的提问句，默认转为中文
        自定义：支持自定义任务，{{输入}} 位置自定插入输入的内容，一般开头定义任务，中间是输入，最后是要求，可点击 调试 按钮对定义的任务进行测试
    普通模式：直接拿输入的内容进行检索
- 检索分流：检索分流功能是让大模型根据文件管理的文件夹名称和描述，预先判断问题所属文件夹，缩小检索范围
- 检索模式：直接返回检索得分top的片段还是利用模型进行相关性分析，可根据业务实际情况选用。智能检索将检索到的文本发送给大模型根据语义选取相关度高的片段。[!检索模式](./检索模式.png)
- 重排序模型：重排序模型将根据候选文档列表与用户问题语义匹配度进行重新排序，从而改进语义排序的结果，其原理是计算用户问题与给定的每个候选文档之间的相关性分数，并返回按相关性从高到低排序的文档列表。[!重排序模型](./重排序模型.png)
- 预览调试：预览每个节点的结果，可定位不准的检索节点

## 常用的11款Embedding模型
https://zhuanlan.zhihu.com/p/714610288

## ES模糊查询
Elasticsearch（关键字检索）：基于倒排索引（Inverted Index），适合处理精确匹配和全文搜索查
询。它通过分析、索引文本数据中的关键词，然后在搜索时匹配这些关键词来找到相关文档。

## 向量检索
向量检索：基于向量空间模型，通常与机器学习模型（如深度学习）结合使用，将文本、图片或其他数
据转换成高维空间中的向量。搜索时，通过计算查询向量与数据集中向量之间的相似度（通常使用余弦
相似度、欧式距离等）来检索最相关的项。

向量检索和ES在检索领域各有其优势，混合搜索结合了这两种搜索技术的优点，同时弥补了两方的缺点。

## OCR
光学字符识别(OCR) 是一种使用自动数据提取将文本图像快速转换为机器可读格式的技术。 OCR 有时称为文本识别。 OCR 程序从扫描的文档、相机图像和纯图像PDF 中提取和重新利用数据。 OCR 软件将图像上的字母单列出来，将其组合成单词，再将单词组合成句子，从而实现对原始内容的访问和编辑。

## function calling
大模型在面对实时性问题、数学计算等问题时可能效果不佳。Function Calling 通过引入外部工具，让大模型可以回答原本无法解决的问题
```py
# API 请求中定义工具
tools = [{
    "name": "search_weather",
    "parameters": {
        "type": "object",
        "properties": {
            "city": {"type": "string"}
        }
    }
}]

# 模型直接返回结构化调用
{
    "tool_calls": [{
        "name": "search_weather",
        "arguments": {"city": "北京"}
    }]
}
```

## ReAct
ReAct Agent（Reasoning + Acting Agent）是一种结合语言模型推理能力与外部工具调用能力的智能体架构。它源于“ReAct: Synergizing Reasoning and Acting in Language Models”这一经典范式，旨在让大模型不仅能“思考”，还能“行动”。

```text
用户：北京今天天气怎么样？

模型输出：
Thought: 用户想知道北京的天气，我需要调用天气查询工具
Action: search_weather
Action Input: {"city": "北京"}
Observation: [系统返回] 晴，25°C
Thought: 我已获得天气信息，可以回答用户了
Answer: 北京今天天气晴朗，气温25°C。
```

Function Calling 和 ReAct 它们并不互斥，实际上可以结合使用

```
┌─────────────────────────────────────────┐
│              Agent 系统                  │
│  ┌─────────────────────────────────┐    │
│  │   ReAct 框架 (思考-行动循环)      │    │
│  │   ┌─────────────────────────┐   │    │
│  │   │  Function Calling       │   │    │
│  │   │  (结构化工具调用)         │   │    │
│  │   └─────────────────────────┘   │    │
│  └─────────────────────────────────┘    │
└─────────────────────────────────────────┘
```

## Topk
Top-k 是用来控制采样策略的，即从所有结果中按照打分排名，取前 k 个字作为候选集，然后从中随机选一个作为下一个输出的字。

当k=1 时，就和贪心策略效果是一样的，总是选排名第一个的那个。

## TopP
Top-p 也是用来控制材料策略的，不过它是挑选评分（概率）加起来达到 p的最小集合作为候选集，然后从中随机选一个作为下一个输出的字。p值为0-1之间，通常设置为较高的值，比如0.75，这样可以过滤掉那些低评分的长尾。