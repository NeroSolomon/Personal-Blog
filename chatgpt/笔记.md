## 笔记

### prompt 编写
1. http://tny.im/mNWAo
2. http://tny.im/Dz9lW
3. https://jerryzou.com/posts/how-to-write-a-prompt-for-chatgpt/
4. https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api
5. https://docs.anthropic.com/claude/docs/prompt-engineering

#### 例子
假设你是一个高级的开发者，你的目标是根据引导帮助开发者做xxx，这里有个引导的例子：

<instructions>
xxx
</instructions>

开发者让你通过引导去做。
我会用<pre>来提供上文。
我会用<next>来提供下文。
你要做的是xxx。

这里是上文
<pre>
</pre>

这里是下文
<next>
</next>

记住！xxxx
记住！最重要的是xxx

再次说明，以下是做xxx的引导：
<instructions>
xxx
</instructions>

### few-shot
少样本学习，通过给大模型一个简单的例子，让他按照这个例子回答

### embedding
在机器学习和自然语言处理中，embedding是一种技术，可以将单词、短语或其他类型的数据映射到连续向量空间中。这些向量通常具有较低的维度，并且能够捕获数据之间的相似性和关系。例如，在大型预训练模型如BERT、GPT等中，使用词嵌入来表示输入文本，帮助模型理解语义和上下文信息。

Embedding 的主要目的是将高维稀疏数据转化为低维密集向量，以便机器学习模型更好地处理。这些向量通过训练过程自动学习，使得语义上相近的词或实体在向量空间中也会比较接近。

Embedding 在自然语言处理任务中不直接节省token，而是通过嵌入(或映射)大量离散的词汇到一个相对较小且连绀的向量空间来提高数据表示的效率。这种技术可以帮助减少模型所需处理和学习的复杂性。

例如，在没有embedding的情况下，文本数据通常需要采用独热编码（one-hot encoding），其中每个单词被表示为一个非常长的向量。在这个向量中，只有对应该单词的索引位置是1，其余都是0。这种表示方式随着词汇表大小的增加而迅速增加其维度，导致计算资源消耗巨大且效率低下。

使用embedding后，每个单词被转换成一个固定长度的连续值向量，大大减少了输入数据的维度，并使得训练更加高效。此外，这些向量捕捉了单词之间的语义关系，比如两个意思相近的单词会在向量空间中距离相近，从而提高了模型对语句、段落更好的理解能力。

总结，虽然embedding并不直接“节省token”，但它有效地优化了数据表示方式，降低了模型处理高维稀疏数据的负担，并利于深度学习模型的训练与推断。

### 节省token的方式
在自然语言处理中，确实存在一些策略可以帮助减少或节省token的数量，并提高模型效率。以下是几种常见的方法：

子词分割（Subword Tokenization）:
使用子词单位而不是完整单词进行tokenization，如Byte-Pair Encoding (BPE) 或 SentencePiece 等方法。这些技术通过将常见字符或字节对合并成更高级的单元来工作。这种方式允许处理未知词汇，同时减少了总的token数量，因为它们将单词分解为更通用、重复利用度更高的部分。

词频裁剪（Vocabulary Pruning）:
根据词频或文档频率来筛选和裁剪词汇表。例如，去除那些出现次数很低的罕见词汇，或者针对特定任务去除与任务无关的高频停用词。这种方法能显著减少模型需要处理的唯独稀疏数据。

知识蒸馏（Knowledge Distillation）:
通过从大型网络到小型网络转移知识，以简化模型结构并减少参数量。虽然这主要影响模型大小和计算需求，间接上也可减轻运行时输入输出处理的负担。

动态Token生成:
针对Transformer等模型，有研究探索动态生成tokens方法，即根据上下文条件动态决定哪些tokens是必要的。这可以进一步节省处理时间和资源，尤其是在推理阶段。

层次化Token:
设计不同级别的token表示（如字符、单词和短语），根据现实场景选择适当级别的抽象粒度，这样可以更有效地管理token使用和存储。

采用以上某种或多种结合的方法，可以有效减少所需处理的token数，由此降低计算成本和提高模型效率。每种方法有其优点及局限性，选择适合具体应用和任务的方案是关键。

### 子词分割
子词分割是一种常用的技术，尤其在处理那些单词长度较长或者不常见的词时非常有效。举个例子：

假设我们有一个单词 "unbelievable"。这个单词比较长，而直接把它作为一个整体来处理可能会导致模型需要记忆和管理大量的稀有或长单词。

使用子词分割方法，如Byte-Pair Encoding (BPE) 或 WordPiece，我们可以将 "unbelievable" 分割成更小的单元。例如，在某些BPE配置下，“unbelievable” 可能会被分割为：

"un"
"##believe"
"##able"
这里，“un” 是一个常见的前缀，"##believe" 和 "##able" 也是常见的后缀（其中“##”通常用于标示这部分是接续到前面单词的）。通过这种方式，模型只需学习较短的、更频繁出现的片段，从而提高了数据压缩率和模型效率，同时保持了对原始文本的良好表达。

### 一些市面上的大模型

1. openai o1: 经过强化学习的模型，在回应之前会生成长串的思维链
2. MiniMax：支持联网的知识库检索模型
3. claude 3.5 sonnet：强大的逻辑推理模型，支持多模态，兼具速度和性能
4. claude 3 Haiku：专注速度的模型
5. claude 3 opus：专注智能的模型
6. claude 3 sonnet：兼顾速度和智能的模型
7. DALL-E-3：支持文生图的模型
8. gpt-4o：多模态模型，支持图像对话
9. internvl v1.5：支持图像对话